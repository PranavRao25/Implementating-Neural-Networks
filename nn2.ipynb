{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "b75ba2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "fbbe4837",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, data, childern = None, grad = None, _op='', label=''):\n",
    "        self.data = data\n",
    "        self.grad = 0\n",
    "        self._backward = lambda: None\n",
    "        self.childern = childern\n",
    "        self._op = _op\n",
    "    \n",
    "    def __add__(self, node):\n",
    "        assert isinstance(node, (Node, float, int))\n",
    "        node = Node(node) if not isinstance(node, Node) else node\n",
    "        out = Node(self.data + node.data, _op='+')\n",
    "        out.childern = [self, node]\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += 1 * out.grad\n",
    "            node.grad += 1 * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def __sub__(self, node):\n",
    "        node = Node(node) if not isinstance(node, Node) else node\n",
    "        out = Node(self.data - node.data, childern=[self, node], _op='-')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += 1 * out.grad\n",
    "            node.grad += 1 * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def __mul__(self, node):\n",
    "        node = Node(node) if not isinstance(node, Node) else node\n",
    "        out = Node(self.data * node.data, childern=[self, node], _op='*')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += node.data * out.grad\n",
    "            node.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def __div__(self, node):\n",
    "        node = Node(node) if not isinstance(node, Node) else node\n",
    "        if node.data == 0:  raise ZeroDivisionError\n",
    "        out = Node(self.data / node.data, childern=[self, node], _op='/')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (1 / node.data) * out.grad\n",
    "            node.grad += - (self.data * node.data ** 2) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), \"only supporting int/float powers\"\n",
    "        out = Node(self.data ** other, childern=[self], _op='**')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other * (self.data ** other - 1) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "\n",
    "    def __rsub__(self, other):\n",
    "        return other + (-self)\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "\n",
    "    def tanh(self):\n",
    "        out = Node(np.tanh(self.data), childern=[self], _op='tanh')\n",
    "\n",
    "        def _backward():\n",
    "            x = self.data\n",
    "            t = (math.exp(2*x) - 1) / (math.exp(2*x) + 1)\n",
    "            self.grad += (1 - t**2) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def relu(self):\n",
    "        x = self.data if self.data > 0.0 else 0.0\n",
    "        out = Node(x, childern=[self], _op='relu')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (out.data > 0) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def sigmoid(self):\n",
    "        x = 1 / (1 + np.exp(-self.data))\n",
    "        out = Node(x, childern=[self], _op='sigmoid')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += x * (1 - x) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def _backward():\n",
    "        def recurse_topo_sort(node):\n",
    "            if not node in visited:\n",
    "                visited.add(node)\n",
    "                topo.append(node)\n",
    "                for child in node.childern:\n",
    "                    recurse_topo_sort(child)\n",
    "        \n",
    "        topo = []\n",
    "        visited = set()\n",
    "        recurse_topo_sort(self)\n",
    "\n",
    "        for node in topo:\n",
    "            node._backward()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data:.4f}, grad={self.grad:.4f}), op={self._op}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "67ddae39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNModule(ABC):\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters:\n",
    "            p.grad = 0\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_parameters(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89074aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron(NNModule):\n",
    "    def __init__(self, input_dim, count = 0):\n",
    "        self.weights = [Node(random.uniform(-1, 1), label=f'w_{count}_{i}') for i in range(input_dim)]\n",
    "        # self.weights = np.random.rand(shape = (output_dim, input_dim))\n",
    "        self.bias = Node(random.uniform(-1, 1), label=f'b_{count}')\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\" fn(<w, x> + b)\"\"\"\n",
    "\n",
    "        if len(x) != len(self.weights) : raise ValueError\n",
    "\n",
    "        # s = 0\n",
    "        # for w, x in zip(self.weights, x):\n",
    "        #     print((w * x).data)\n",
    "        #     s += w * x\n",
    "        # ip = s\n",
    "        # ip = sum(w * x for w, x in zip(self.weights, x))\n",
    "        # o = ip + self.bias\n",
    "        # self.out = o.relu()\n",
    "        # self.out = sum(sum(w * x for w, x in zip(self.weights, x)), self.bias).relu()\n",
    "\n",
    "        self.out = sum((wi*xi for wi, xi in zip(self.weights, x)), self.bias).relu()\n",
    "        return self.out\n",
    "\n",
    "    def _backward(self, grad):\n",
    "        x\n",
    "\n",
    "    def get_parameters(self):\n",
    "        return self.weights + [self.bias]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Neuron(inputs={len(self.w)}, nonlin={self.nonlin})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13a4099",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(NNModule):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.input_dim, self.output_dim = input_dim, output_dim\n",
    "        self.neurons = [Neuron(input_dim, count=i) for i in range(output_dim)]\n",
    "\n",
    "    def __call__(self, x) -> list:\n",
    "        if len(x) != self.input_dim:    raise ValueError\n",
    "        self.out = [n(x) for n in self.neurons]\n",
    "        return self.out\n",
    "\n",
    "    def get_parameters(self):\n",
    "        total_parameters = []\n",
    "\n",
    "        for n in self.neurons:\n",
    "            total_parameters += n.get_parameters()\n",
    "        return total_parameters\n",
    "\n",
    "    def _backward(self, grad):\n",
    "        cul_grad = []\n",
    "        for neuron in self.neurons:\n",
    "            cul_grad += neuron._backward(grad)\n",
    "        return cul_grad\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Layer(neurons={len(self.neurons)})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75616a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(NNModule):\n",
    "    def __init__(self, layers = None):\n",
    "        self.layers = layers\n",
    "        self.input_dim = self.layers[0].input_dim if self.layers else 0\n",
    "        self.output_dim = self.layers[-1].output_dim if self.layers else 0\n",
    "        self.n_layers = len(layers) if layers else 0\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, hidden_dims):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.layers = [Layer(input_dim, hidden_dims[0])]\n",
    "        for i in range(1, len(hidden_dims)):\n",
    "            self.layers.append(Layer(hidden_dims[i - 1], hidden_dims[i]))\n",
    "        self.layers.append(Layer(hidden_dims[-1], output_dim))\n",
    "        self.n_layers = len(self.layers)\n",
    "\n",
    "    def add_layer(self, layer):\n",
    "        if self.layers[-1].output_dim != layer.input_dim:   raise ValueError\n",
    "\n",
    "        self.layers.append(layer)\n",
    "        self.output_dim = layer.output_dim\n",
    "\n",
    "    def __call__(self, x) -> list:\n",
    "        if len(x) != self.input_dim:    raise ValueError\n",
    "        for layer in self.layers:\n",
    "            out = layer(x)\n",
    "            x = out\n",
    "        self.out = out\n",
    "        return self.out\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        total_parameters = []\n",
    "        for layer in self.layers:\n",
    "            total_parameters += layer.get_parameters()\n",
    "        return total_parameters\n",
    "\n",
    "    def _backward(self, train_loss):\n",
    "        grad = self.layers[-1]._backward(train_loss)\n",
    "        for layer in self.layers[:-1:-1]:\n",
    "            grad = layer._backward(grad)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        s = f\"MLP(layers=[\\n\"\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            s += f\"  Layer {i}: {len(layer.neurons)} neurons (input {layer.neurons[0].w[0].label.count('w')})\\n\"\n",
    "        s += \"])\"\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "1461085e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = lambda pred, y: sum((p - z) ** 2 for p, z in zip(pred, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdbd900",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, model, epochs, lr = 1e-2, loss = mse):\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.model = model\n",
    "        self.loss = loss\n",
    "    \n",
    "    def train(self, X, y):\n",
    "        if X.shape[1] != self.model.input_dim:   raise ValueError\n",
    "        if X.shape[0] != y.shape[0]:    raise ValueError\n",
    "\n",
    "        train_err_schedule = []\n",
    "        for epoch in range(self.epochs):\n",
    "            train_loss = 0\n",
    "            for i in range(X.shape[0]):\n",
    "                pred = self.model(X[i])\n",
    "                e = mse(pred, y[i])\n",
    "                train_loss += e\n",
    "            print(epoch, train_loss.data)\n",
    "            train_err_schedule.append(train_loss.data)\n",
    "\n",
    "            train_loss._backward()\n",
    "            \n",
    "            for p in self.model.get_parameters():\n",
    "                p -= train_loss * self.lr\n",
    "        return train_err_schedule\n",
    "    \n",
    "    def test(self, X):\n",
    "        if X.shape[1] != self.model.input_dim:   raise ValueError\n",
    "        return [self.model(x) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "cb8e7da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.array([\n",
    "    [2.0, 3.0],\n",
    "    [3.0, -1.0],\n",
    "    [0.5, 1.0],\n",
    "    [1.0, 1.0]\n",
    "])\n",
    "ys = np.array([1.0, -1.0, -1.0, 1.0])[:, np.newaxis]\n",
    "model = MLP(2, 1, [4, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "810d6f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer = ModelTrainer(model, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8174dbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = model_trainer.train(xs, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bd75ba",
   "metadata": {},
   "source": [
    "1. Prior parameter:  Value(data=-0.8217, grad=0.0000), op=\n",
    "2. Post parameter:  Value(data=-0.8982, grad=0.0000), op=-\n",
    "3. Changed? Value(data=-0.8217, grad=0.0000), op="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d897f7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
